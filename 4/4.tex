% Template by Jennifer Pan, August 2011

\documentclass[10pt,letter]{article}
	% basic article document class
	% use percent signs to make comments to yourself -- they will not show up.

\usepackage{amsmath}
\usepackage{amssymb}
	% packages that allow mathematical formatting

\usepackage{graphicx}
	% package that allows you to include graphics

\usepackage{setspace}
	% package that allows you to change spacing

\onehalfspacing
	% text become 1.5 spaced

\usepackage{fullpage}
	% package that specifies normal margins
	

\begin{document}
	% line of code telling latex that your document is beginning


\title{Problem Set 4}

\author{Tom O'Connell}

\date{August 20, 2016}
	% Note: when you omit this command, the current dateis automatically included
 
\maketitle 
	% tells latex to follow your header (e.g., title, author) commands.


\section*{1 Perceptrons}

\paragraph{1)}

\subparagraph{a)} Write out the functional form and learning algorithm for the perceptron.

% TODO other parens
\[ y\hat = \text{sign}((\sum_{i=0}^{d}{w_ix_i}))  \]

Where $w_0$ is the bias weight, and $x_0$ is defined to be $1$. $d$ is the number of feature 
dimensions of the input vectors.\\

The learning algorithm alternates between calculated the estimated label for an input point, as 
above, and updated the weights.  The formula for updating the weights is:

\[ w_i(t + 1) = w_i(t) + x_{j,i} (y_j^\text{truth} - \hat{y}_j)  \]

\subparagraph{b)} Is this a generative or discriminative model?

This model is discriminative, because it changes the position of a linear decision boundary to 
separate the data without estimating the class conditional probability distributions. The model 
can use this boundary to discriminate input data into their underlying classes, but does not reason
 about the chances of getting different types of inputs belonging to any one given class.

\subparagraph{c)} What classes of functions can the perceptron represent? How does this limit the types of data we can accurately model? \\

Single layer perceptrons can only separate linearly separable data, because the nonlinearity is 
monotonic, and thus will not change the decision threshold set by the coefficients of the linear 
transformation.\\

This prevents us from representing many classes of problems with single layer perceptrons, but 
we can stack perceptrons to represent more complicated functions, beyond those that are simply 
linearly separable.

\paragraph{2)}

\subparagraph{a)}
\subparagraph{b)} Plot a line that separates the classes perfectly.  What is the equation of this 
line? Is this the only solution?\\

The equation of this line is $-x_1 + 0.8 = x_2$.\\

This line is not the only solution that could correctly separate data generated by the OR function.  There are infinitely many lines that can perform this function, and you can see this just by shifting the shown line up and down by any real number such that it still separates the classes.

\subparagraph{c)}

\begin{center}
    \begin{tabular}{ |c|c|c|c|c|c| } 
         \hline
          $x_1$ & $x_2$ & $y$ & $w_1$ & $w_2$ & $b$\\ 
          $0$ & $0$ & $0$ & $0$ & $0$ & $0$\\ 
          $1$ & $0$ & $1$ & $1$ & $0$ & $1$\\ 
          $0$ & $1$ & $1$ & $1$ & $0$ & $1$\\ 
          $1$ & $1$ & $1$ & $1$ & $0$ & $1$\\ 
          \hline
      \end{tabular}
  \end{center}

At this point, however, my perceptron, trained with the algorithm described above, has not yet 
converged.  Circularly looping through the data, it takes $9$ iterations before the training 
converges, and after then the weights never change on any future iterations.

\paragraph{3)}

\subparagraph{a)}
% TODO plot

\subparagraph{b)}
% TODO draw

% TODO debug
\subparagraph{c)} Training and test accuracies are each approximately $93\%$, though not identical.

\section*{2 Logistic Regression}

\paragraph{1)} Write out the functional form and loss function $\mathcal{L}$ for logistic regression.\\

The functional form for classification with logistic regression is:

\[ \hat{y} = \text{sign}(\frac{1}{1 + e^{-\theta(x)}}) \]
\[ \theta(x) = \sum{i=0}{d}{w_ix_i} \]

The expression for the loss function $\mathcal{L}$ for a single training example is:

\[ \mathcal{L}(w) = \ln(1 + e^{-y\theta(x)}) \]

The loss function over all training examples is a sum of the above, normalized by the number of 
training examples.


\paragraph{2)} Derive the gradient of the loss function with respect to the weights.

The gradient of the loss function with respect to the weights if the sum of the loss on one example 
over all the training examples.

\[ \frac{\partial \mathcal{L}}{\partial w} = \frac{1}{1 + e^{-y\theta(x)}} -\frac{\partial \theta(x)}
{\partial w} \]

Where for each $i$,

\[ \frac{\partial\theta(x)}{\partial w_i} = x_i \]

\[ \frac{\partial \mathcal{L}}{\partial w} = \frac{yx}{1 + e^{y\theta(x)}} \]

Since the gradient is linear with respect to summation, the gradient over all training examples is just the sum of the individuals.

\paragraph{3)} Plots for the logistic regression programming exercises.

% TODO list accuracies

\subparagraph{a)} Logistic regression on dataset B, which is linearly separable.

\subparagraph{b)} Logistic regression on dataset C, which is not linearly separable.

\subparagraph{c)} Logistic regression on the same dataset C, but with inputs expanded into all 
    pairwise products before linearly combining into the logistic function input.

\end{document}
	% line of code telling latex that your document is ending. If you leave this out, you'll get an error
