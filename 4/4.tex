% Template by Jennifer Pan, August 2011

\documentclass[10pt,letter]{article}
	% basic article document class
	% use percent signs to make comments to yourself -- they will not show up.

\usepackage{amsmath}
\usepackage{amssymb}
	% packages that allow mathematical formatting

\usepackage{graphicx}
	% package that allows you to include graphics
\graphicspath{{figures/}}

\usepackage[export]{adjustbox}

\usepackage{setspace}
	% package that allows you to change spacing

\onehalfspacing
	% text become 1.5 spaced

\usepackage{fullpage}
	% package that specifies normal margins
	

\begin{document}
	% line of code telling latex that your document is beginning

\title{Problem Set 4}

\author{Tom O'Connell \\ \large Collaborators: Matt Rosenberg}

\date{August 20, 2016}
	% Note: when you omit this command, the current dateis automatically included
 
\maketitle 
	% tells latex to follow your header (e.g., title, author) commands.


\section*{1 Perceptrons}

\paragraph{1)}

\subparagraph{a)} Write out the functional form and learning algorithm for the perceptron.

% TODO other parens
\[ y\hat = \text{sign}((\sum_{i=0}^{d}{w_ix_i}))  \]

Where $w_0$ is the bias weight, and $x_0$ is defined to be $1$. $d$ is the number of feature 
dimensions of the input vectors.\\

The learning algorithm alternates between calculated the estimated label for an input point, as 
above, and updated the weights.  The formula for updating the weights is:

\[ w_i(t + 1) = w_i(t) + x_{j,i} (y_j^\text{truth} - \hat{y}_j)  \]

\subparagraph{b)} Is this a generative or discriminative model?

This model is discriminative, because it changes the position of a linear decision boundary to 
separate the data without estimating the class conditional probability distributions. The model 
can use this boundary to discriminate input data into their underlying classes, but does not reason
 about the chances of getting different types of inputs belonging to any one given class.

\subparagraph{c)} What classes of functions can the perceptron represent? How does this limit the types of data we can accurately model? \\

Single layer perceptrons can only separate linearly separable data, because the nonlinearity is 
monotonic, and thus will not change the decision threshold set by the coefficients of the linear 
transformation.\\

This prevents us from representing many classes of problems with single layer perceptrons, but 
we can stack perceptrons to represent more complicated functions, beyond those that are simply 
linearly separable.

\paragraph{2)}

\subparagraph{a)}
\subparagraph{b)} Plot a line that separates the classes perfectly.  What is the equation of this 
line? Is this the only solution?\\

See attached figures (at end of text) for plot.\\
\begin{figure}[b]
\vspace*{-1cm}
\centering
\includegraphics[width=0.9\linewidth, height=10cm]{fig1.2.eps}
\end{figure}

The equation of this line is $-x_1 + 0.8 = x_2$.\\

This line is not the only solution that could correctly separate data generated by the OR function.  There are infinitely many lines that can perform this function, and you can see this just by shifting the shown line up and down by any real number such that it still separates the classes.

\subparagraph{c)}

\begin{center}
    \begin{tabular}{ |c|c|c|c|c|c| } 
         \hline
          $x_1$ & $x_2$ & $y$ & $w_1$ & $w_2$ & $b$\\ 
          $0$ & $0$ & $0$ & $0$ & $0$ & $0$\\ 
          $1$ & $0$ & $1$ & $1$ & $0$ & $1$\\ 
          $0$ & $1$ & $1$ & $1$ & $0$ & $1$\\ 
          $1$ & $1$ & $1$ & $1$ & $0$ & $1$\\ 
          \hline
      \end{tabular}
  \end{center}

At this point, however, my perceptron, trained with the algorithm described above, has not yet 
converged.  Circularly looping through the data, it takes $9$ iterations before the training 
converges, and after then the weights never change on any future iterations.

\paragraph{3)}

\subparagraph{a)} See attached figures.
\begin{figure}[b]
\vspace*{-1cm}
\centering
\includegraphics[width=0.9\linewidth, height=10cm]{fig1.3a.eps}
\caption{Linear threshold units circumscribing $c=1$ regions.}
%\begin{subfigure}{0.5\textwidth}
%\includegraphics[width=0.9\linewidth, height=8cm]{fig1.3b.eps}
%\caption{Architecture of LTU $\rightarrow$ AND $\rightarrow$ OR multilayer perceptron.}
%\end{subfigure}
\end{figure}

\subparagraph{b)} See attached figures.
\begin{figure}[b]
\vspace*{-2cm}
\centering
\includegraphics[width=0.8\linewidth, right]{fig1.3b.eps}
\end{figure}

% TODO debug
% TODO directly fill in this value from the program if possible
\subparagraph{c)} Training and test accuracies are each approximately $93\%$, though not identical.

\section*{2 Logistic Regression}

\paragraph{1)} Write out the functional form and loss function $\mathcal{L}$ for logistic regression.\\

The functional form for classification with logistic regression is:

\[ \hat{y} = \text{sign}(\frac{1}{1 + e^{-\theta(x)}}) \]
\[ \theta(x) = \sum{i=0}{d}{w_ix_i} \]

The expression for the loss function $\mathcal{L}$ for a single training example is:

\[ \mathcal{L}(w) = \ln(1 + e^{-y\theta(x)}) \]

The loss function over all training examples is a sum of the above, normalized by the number of 
training examples.

\paragraph{2)} Derive the gradient of the loss function with respect to the weights.

The gradient of the loss function with respect to the weights if the sum of the loss on one example 
over all the training examples.

\[ \frac{\partial \mathcal{L}}{\partial w} = \frac{1}{1 + e^{-y\theta(x)}} -\frac{\partial \theta(x)}
{\partial w} \]

Where for each $i$,

% TODO wrong i think. doesn't lead to next step.
\[ \frac{\partial\theta(x)}{\partial w_i} = x_i \]

\[ \frac{\partial \mathcal{L}}{\partial w} = \frac{yx}{1 + e^{y\theta(x)}} \]

Since the gradient is linear with respect to summation, the gradient over all training examples is just the sum of the individuals.

\paragraph{3)} Plots for the logistic regression programming exercises.

\subparagraph{a)} Logistic regression on dataset B, which is linearly separable.\\

Since the data is linearly separable, we would expect perfect classification performance with 
this generalized linear model, and we do get $100\%$ accuracy on the training and test sets.

\begin{figure}[b]
\vspace*{-2cm}
\centering
\includegraphics[width=0.9\linewidth, height=10cm]{fig2.3a.eps}
\end{figure}

\subparagraph{b)} Logistic regression on dataset C, which is not linearly separable.\\

Since the dataset is not linearly separable, training directly on the feature vectors yields poor 
classification accuracy, with a training set accuracy of $0.813$ and a test set accuracy of $0.744$.

\begin{figure}[b]
\vspace*{1cm}
\centering
\includegraphics[width=0.9\linewidth, height=10cm]{fig2.3b.eps}
\end{figure}

\subparagraph{c)} Logistic regression on the same dataset C, but with inputs expanded into all 
    pairwise products before linearly combining into the logistic function input.\\

Now, both the training and testing accuracies are $100\%$, with the a product between the two 
original features and each of the original features squared concatenated on to the feature vectors.

\begin{figure}[b]
\vspace*{-4cm}
\centering
\includegraphics[width=0.9\linewidth, height=10cm]{fig2.3c.eps}
\end{figure}

\end{document}
	% line of code telling latex that your document is ending. If you leave this out, you'll get an error
